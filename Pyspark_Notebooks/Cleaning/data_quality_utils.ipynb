{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a97c8b2-097b-440c-bbbd-76cac2d9cfac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import *\n",
    "import re\n",
    "from pyspark.sql import functions as F\n",
    "from functools import reduce\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 1. Advanced Text Similarity Functions\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "@udf(DoubleType())\n",
    "def advanced_name_similarity(name1, name2):\n",
    "    \"\"\"\n",
    "    Advanced similarity considering:\n",
    "    - Levenshtein distance\n",
    "    - Token overlap\n",
    "    - Soundex matching\n",
    "    \"\"\"\n",
    "    if not name1 or not name2:\n",
    "        return 0.0\n",
    "    \n",
    "    name1 = name1.lower().strip()\n",
    "    name2 = name2.lower().strip()\n",
    "    \n",
    "    # Exact match\n",
    "    if name1 == name2:\n",
    "        return 1.0\n",
    "    \n",
    "    # Token-based similarity\n",
    "    tokens1 = set(name1.split())\n",
    "    tokens2 = set(name2.split())\n",
    "    \n",
    "    if len(tokens1) == 0 or len(tokens2) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Jaccard similarity\n",
    "    intersection = len(tokens1 & tokens2)\n",
    "    union = len(tokens1 | tokens2)\n",
    "    jaccard = intersection / union if union > 0 else 0.0\n",
    "    \n",
    "    # Character-level similarity (simplified Levenshtein)\n",
    "    max_len = max(len(name1), len(name2))\n",
    "    char_sim = 1.0 - (levenshtein(name1, name2) / max_len) if max_len > 0 else 0.0\n",
    "    \n",
    "    # Combined score (weighted)\n",
    "    final_score = (jaccard * 0.6) + (char_sim * 0.4)\n",
    "    \n",
    "    return final_score\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "@udf(DoubleType())\n",
    "def url_domain_similarity(url1, url2):\n",
    "    \"\"\"\n",
    "    Compare domain names from URLs\n",
    "    \"\"\"\n",
    "    if not url1 or not url2:\n",
    "        return 0.0\n",
    "    \n",
    "    # Extract domain\n",
    "    import re\n",
    "    \n",
    "    def extract_domain(url):\n",
    "        # Remove protocol\n",
    "        url = re.sub(r'^https?://', '', url)\n",
    "        # Remove www\n",
    "        url = re.sub(r'^www\\.', '', url)\n",
    "        # Get domain (before first /)\n",
    "        domain = url.split('/')[0]\n",
    "        # Remove port\n",
    "        domain = domain.split(':')[0]\n",
    "        return domain.lower()\n",
    "    \n",
    "    domain1 = extract_domain(url1)\n",
    "    domain2 = extract_domain(url2)\n",
    "    \n",
    "    if domain1 == domain2:\n",
    "        return 1.0\n",
    "    \n",
    "    # Check if one is subdomain of other\n",
    "    if domain1 in domain2 or domain2 in domain1:\n",
    "        return 0.9\n",
    "    \n",
    "    return 0.0\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 2. Fuzzy Matching Pipeline for Unmatched Records\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def find_fuzzy_matches(source_df, target_df, source_name_col, target_name_col, \n",
    "                       source_url_col=None, target_url_col=None, threshold=0.85):\n",
    "    \"\"\"\n",
    "    Find fuzzy matches between two dataframes\n",
    "    \n",
    "    Parameters:\n",
    "    - source_df: DataFrame to match FROM\n",
    "    - target_df: DataFrame to match TO\n",
    "    - source_name_col: company name column in source\n",
    "    - target_name_col: company name column in target\n",
    "    - source_url_col: optional URL column in source\n",
    "    - target_url_col: optional URL column in target\n",
    "    - threshold: minimum similarity threshold (0-1)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with matched pairs and similarity scores\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cross join for comparison (use broadcast for smaller df)\n",
    "    if source_df.count() < target_df.count():\n",
    "        matches = source_df.crossJoin(broadcast(target_df))\n",
    "    else:\n",
    "        matches = broadcast(source_df).crossJoin(target_df)\n",
    "    \n",
    "    # Calculate name similarity\n",
    "    matches = matches.withColumn(\n",
    "        \"name_similarity\",\n",
    "        advanced_name_similarity(col(source_name_col), col(target_name_col))\n",
    "    )\n",
    "    \n",
    "    # Calculate URL similarity if URLs provided\n",
    "    if source_url_col and target_url_col:\n",
    "        matches = matches.withColumn(\n",
    "            \"url_similarity\",\n",
    "            url_domain_similarity(col(source_url_col), col(target_url_col))\n",
    "        )\n",
    "        \n",
    "        # Combined score: 70% name, 30% URL\n",
    "        matches = matches.withColumn(\n",
    "            \"combined_similarity\",\n",
    "            (col(\"name_similarity\") * 0.7) + (col(\"url_similarity\") * 0.3)\n",
    "        )\n",
    "    else:\n",
    "        matches = matches.withColumn(\"combined_similarity\", col(\"name_similarity\"))\n",
    "    \n",
    "    # Filter by threshold\n",
    "    matches = matches.filter(col(\"combined_similarity\") >= threshold)\n",
    "    \n",
    "    # Rank matches\n",
    "    window = Window.partitionBy(source_name_col).orderBy(col(\"combined_similarity\").desc())\n",
    "    matches = matches.withColumn(\"match_rank\", row_number().over(window))\n",
    "    \n",
    "    return matches\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 3. Data Quality Checks\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def check_uen_validity(df, uen_col=\"uen\"):\n",
    "    \"\"\"\n",
    "    Check UEN format validity for Singapore companies\n",
    "    UEN format: XXXXXXXXX + letter (9 digits + 1 letter) or similar patterns\n",
    "    \"\"\"\n",
    "    \n",
    "    return df.withColumn(\n",
    "        \"uen_is_valid\",\n",
    "        when(\n",
    "            col(uen_col).rlike(r'^[0-9]{8,9}[A-Z]$') |\n",
    "            col(uen_col).rlike(r'^[0-9]{10}[A-Z]$') |\n",
    "            col(uen_col).rlike(r'^[STF][0-9]{8}[A-Z]$'),\n",
    "            True\n",
    "        ).otherwise(False)\n",
    "    )\n",
    "\n",
    "def check_url_validity(df, url_col=\"website\"):\n",
    "    \"\"\"\n",
    "    Check if URL is valid format\n",
    "    \"\"\"\n",
    "    \n",
    "    return df.withColumn(\n",
    "        \"url_is_valid\",\n",
    "        when(\n",
    "            col(url_col).isNotNull() &\n",
    "            (col(url_col).rlike(r'^[a-z0-9]+([\\-\\.]{1}[a-z0-9]+)*\\.[a-z]{2,}') |\n",
    "             col(url_col).rlike(r'^https?://[a-z0-9]+([\\-\\.]{1}[a-z0-9]+)*\\.[a-z]{2,}')),\n",
    "            True\n",
    "        ).otherwise(False)\n",
    "    )\n",
    "\n",
    "def check_email_validity(df, email_col=\"contact_email\"):\n",
    "    \"\"\"\n",
    "    Check if email is valid format\n",
    "    \"\"\"\n",
    "    \n",
    "    return df.withColumn(\n",
    "        \"email_is_valid\",\n",
    "        when(\n",
    "            col(email_col).isNotNull() &\n",
    "            col(email_col).rlike(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'),\n",
    "            True\n",
    "        ).otherwise(False)\n",
    "    )\n",
    "\n",
    "def check_phone_validity(df, phone_col=\"contact_phone\"):\n",
    "    \"\"\"\n",
    "    Check if phone number is valid Singapore format\n",
    "    Singapore: +65 XXXX XXXX or 6/8/9 followed by 7 digits\n",
    "    \"\"\"\n",
    "    \n",
    "    return df.withColumn(\n",
    "        \"phone_is_valid\",\n",
    "        when(\n",
    "            col(phone_col).isNotNull() &\n",
    "            (col(phone_col).rlike(r'^\\+65\\s*[689]\\d{7}$') |\n",
    "             col(phone_col).rlike(r'^[689]\\d{7}$')),\n",
    "            True\n",
    "        ).otherwise(False)\n",
    "    )\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def run_all_quality_checks(df):\n",
    "    \"\"\"\n",
    "    Run all quality checks on unified dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    df = check_uen_validity(df, \"uen\")\n",
    "    df = check_url_validity(df, \"website\")\n",
    "    df = check_email_validity(df, \"contact_email\")\n",
    "    df = check_phone_validity(df, \"contact_phone\")\n",
    "    \n",
    "    # Add overall quality score\n",
    "    df = df.withColumn(\n",
    "        \"data_quality_score\",\n",
    "        (\n",
    "            when(col(\"uen_is_valid\"), 1).otherwise(0) +\n",
    "            when(col(\"url_is_valid\"), 1).otherwise(0) +\n",
    "            when(col(\"email_is_valid\"), 1).otherwise(0) +\n",
    "            when(col(\"phone_is_valid\"), 1).otherwise(0)\n",
    "        ) / 4.0 * 100\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 4. Deduplication Functions\n",
    "\n",
    "# COMMAND ----------\n",
    "def calculate_data_completeness_score(df):\n",
    "    \"\"\"\n",
    "    ✅ Calculates a completeness percentage per record.\n",
    "    Works with Spark Connect (Databricks) by properly summing columns.\n",
    "    \"\"\"\n",
    "    important_fields = [\n",
    "        \"website\",\n",
    "        \"linkedin\",\n",
    "        \"facebook\",\n",
    "        \"instagram\",\n",
    "        \"industry\",\n",
    "        \"revenue\",\n",
    "        \"contact_email\",\n",
    "        \"contact_phone\",\n",
    "        \"products_offered\",\n",
    "        \"services_offered\"\n",
    "    ]\n",
    "\n",
    "    total_fields = len(important_fields)\n",
    "\n",
    "    # Build column expressions safely\n",
    "    cols_to_sum = [F.when(F.col(c).isNotNull(), F.lit(1)).otherwise(F.lit(0)) for c in important_fields]\n",
    "\n",
    "    # Handle case where only one column exists (reduce fails with 1)\n",
    "    if len(cols_to_sum) > 1:\n",
    "        completeness_expr = reduce(lambda a, b: a + b, cols_to_sum)\n",
    "    else:\n",
    "        completeness_expr = cols_to_sum[0]\n",
    "\n",
    "    # Calculate percentage completeness\n",
    "    df = df.withColumn(\n",
    "        \"data_completeness_score\",\n",
    "        (completeness_expr / F.lit(total_fields)) * 100\n",
    "    )\n",
    "\n",
    "    return df\n",
    "def find_duplicates(df, match_cols, similarity_threshold=0.95):\n",
    "    \"\"\"\n",
    "    Find potential duplicate records based on similarity\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame to check\n",
    "    - match_cols: list of columns to check for duplicates\n",
    "    - similarity_threshold: threshold for considering duplicates\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with potential duplicate groups\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create self-join for comparison\n",
    "    df1 = df.alias(\"df1\")\n",
    "    df2 = df.alias(\"df2\")\n",
    "    \n",
    "    # Join on similarity\n",
    "    duplicates = df1.join(\n",
    "        df2,\n",
    "        (col(\"df1.uen\") < col(\"df2.uen\")) &  # Avoid self-comparison and duplicates\n",
    "        (\n",
    "            advanced_name_similarity(col(\"df1.company_name\"), col(\"df2.company_name\")) >= similarity_threshold\n",
    "        ),\n",
    "        \"inner\"\n",
    "    )\n",
    "    \n",
    "    duplicates = duplicates.select(\n",
    "        col(\"df1.uen\").alias(\"uen1\"),\n",
    "        col(\"df1.company_name\").alias(\"name1\"),\n",
    "        col(\"df2.uen\").alias(\"uen2\"),\n",
    "        col(\"df2.company_name\").alias(\"name2\"),\n",
    "        advanced_name_similarity(col(\"df1.company_name\"), col(\"df2.company_name\")).alias(\"similarity\")\n",
    "    )\n",
    "    \n",
    "    return duplicates\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 5. Missing Data Imputation Strategies\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def impute_company_size(df):\n",
    "    \"\"\"\n",
    "    Impute company size based on available data\n",
    "    \"\"\"\n",
    "    \n",
    "    return df.withColumn(\n",
    "        \"company_size_imputed\",\n",
    "        when(col(\"company_size\") == \"Unknown\", \n",
    "             when(col(\"revenue\").isNotNull() & (col(\"revenue\") > 50000000), \"Large\")\n",
    "             .when(col(\"revenue\").isNotNull() & (col(\"revenue\") > 5000000), \"Medium\")\n",
    "             .when(col(\"revenue\").isNotNull(), \"Small\")\n",
    "             .otherwise(\"Unknown\")\n",
    "        ).otherwise(col(\"company_size\"))\n",
    "    )\n",
    "\n",
    "def impute_industry(df):\n",
    "    \"\"\"\n",
    "    Impute industry from keywords or description\n",
    "    \"\"\"\n",
    "    \n",
    "    # Common industry keywords\n",
    "    df = df.withColumn(\n",
    "        \"industry_imputed\",\n",
    "        when(col(\"industry\").isNull(),\n",
    "             when(lower(col(\"keywords\")).contains(\"tech\") | lower(col(\"keywords\")).contains(\"software\"), \"Technology\")\n",
    "             .when(lower(col(\"keywords\")).contains(\"finance\") | lower(col(\"keywords\")).contains(\"bank\"), \"Finance\")\n",
    "             .when(lower(col(\"keywords\")).contains(\"retail\") | lower(col(\"keywords\")).contains(\"ecommerce\"), \"Retail\")\n",
    "             .when(lower(col(\"keywords\")).contains(\"food\") | lower(col(\"keywords\")).contains(\"restaurant\"), \"Food & Beverage\")\n",
    "             .when(lower(col(\"keywords\")).contains(\"health\") | lower(col(\"keywords\")).contains(\"medical\"), \"Healthcare\")\n",
    "             .otherwise(\"Unknown\")\n",
    "        ).otherwise(col(\"industry\"))\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 6. Export Functions\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def export_matching_report(unified_df, output_path):\n",
    "    \"\"\"\n",
    "    Generate comprehensive matching quality report\n",
    "    \"\"\"\n",
    "    \n",
    "    report = unified_df.select(\n",
    "        \"uen\",\n",
    "        \"company_name\",\n",
    "        \"website\",\n",
    "        \"data_completeness_score\",\n",
    "        \"data_quality_score\",\n",
    "        \"entity_status_description\"\n",
    "    ).orderBy(col(\"data_completeness_score\").desc())\n",
    "    \n",
    "    # Save as CSV for easy viewing\n",
    "    report.coalesce(1).write \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(f\"{output_path}/matching_quality_report\")\n",
    "    \n",
    "    print(f\"✓ Report saved to: {output_path}/matching_quality_report\")\n",
    "\n",
    "def export_statistics(unified_df, output_path):\n",
    "    \"\"\"\n",
    "    Export summary statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    stats = unified_df.select(\n",
    "        count(\"*\").alias(\"total_records\"),\n",
    "        countDistinct(\"uen\").alias(\"unique_companies\"),\n",
    "        (count(\"website\") / count(\"*\") * 100).alias(\"website_coverage\"),\n",
    "        (count(\"linkedin\") / count(\"*\") * 100).alias(\"linkedin_coverage\"),\n",
    "        (count(\"revenue\") / count(\"*\") * 100).alias(\"revenue_coverage\"),\n",
    "        avg(\"data_completeness_score\").alias(\"avg_completeness\"),\n",
    "        avg(\"data_quality_score\").alias(\"avg_quality\")\n",
    "    )\n",
    "    \n",
    "    stats.coalesce(1).write \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(f\"{output_path}/summary_statistics\")\n",
    "    \n",
    "    print(f\"✓ Statistics saved to: {output_path}/summary_statistics\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data_quality_utils",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
